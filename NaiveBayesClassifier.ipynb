{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('twitter_samples')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATIONS_PATH = \"claws_classifier/annotations.csv\"\n",
    "APRIL_TWEETS_CLAWS_CLASSIFIED_DIR_PATH = '/home/dhruv/Downloads/kaggle-covid-en-tweets-april_classified_claws/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/env3_bertviz/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (4,25) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pickle, os\n",
    "import pandas as pd \n",
    "\n",
    "train_data = pd.read_csv(ANNOTATIONS_PATH) \n",
    "\n",
    "test_data = []\n",
    "for csv_file in os.listdir(APRIL_TWEETS_CLAWS_CLASSIFIED_DIR_PATH):\n",
    "    df = pd.read_csv(APRIL_TWEETS_CLAWS_CLASSIFIED_DIR_PATH + csv_file, usecols = ['text','Label'],index_col=None, header=0)\n",
    "    test_data.append(df)\n",
    "    \n",
    "test_data = pd.concat(test_data, axis=0, ignore_index=True)\n",
    "test_data = test_data.rename(columns={'text': 'Text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "\n",
    "import re, string, random\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "def get_tweet_tokens(tweet_text):\n",
    "    return TweetTokenizer().tokenize(tweet_text)\n",
    "\n",
    "def get_dataset(train=False,test=False):\n",
    "    if train:\n",
    "        counterhate_tweets = train_data[train_data['Label'] == 'Counterhate']['Text']\n",
    "        hate_tweets = train_data[train_data['Label'] == 'Hate']['Text']\n",
    "    elif test:\n",
    "        counterhate_tweets = test_data[test_data['Label'] == 'Counterhate']['Text']\n",
    "        hate_tweets = test_data[test_data['Label'] == 'Hate']['Text']\n",
    "\n",
    "    counterhate_tweet_tokens = counterhate_tweets.apply(get_tweet_tokens)\n",
    "    hate_tweet_tokens = hate_tweets.apply(get_tweet_tokens)\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    counterhate_cleaned_tokens_list = []\n",
    "    hate_cleaned_tokens_list = []\n",
    "\n",
    "    for tokens in counterhate_tweet_tokens.tolist():\n",
    "        counterhate_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "    for tokens in hate_tweet_tokens.tolist():\n",
    "        hate_cleaned_tokens_list.append(remove_noise(tokens, stop_words)) \n",
    "\n",
    "\n",
    "    counterhate_tokens_for_model = get_tweets_for_model(counterhate_cleaned_tokens_list)\n",
    "    hate_tokens_for_model = get_tweets_for_model(hate_cleaned_tokens_list)\n",
    "\n",
    "\n",
    "    counterhate_dataset = [(tweet_dict, \"Counterhate\")\n",
    "                         for tweet_dict in counterhate_tokens_for_model]\n",
    "\n",
    "    hate_dataset = [(tweet_dict, \"Hate\")\n",
    "                         for tweet_dict in hate_tokens_for_model]\n",
    "\n",
    "    dataset = counterhate_dataset + hate_dataset\n",
    "\n",
    "    random.shuffle(dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def classify_tweet(tweet_text):\n",
    "    custom_tokens = remove_noise(word_tokenize(tweet_text))\n",
    "    return classifier.classify(dict([token, True] for token in custom_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.8570035707954516\n",
      "Most Informative Features\n",
      "           #iamnotavirus = True           Counte : Hate   =     29.4 : 1.0\n",
      "                   stand = True           Counte : Hate   =     23.3 : 1.0\n",
      "         #racismisavirus = True           Counte : Hate   =     21.6 : 1.0\n",
      "                 #racism = True           Counte : Hate   =     18.2 : 1.0\n",
      "                 calling = True           Counte : Hate   =     15.7 : 1.0\n",
      "                     ccp = True             Hate : Counte =     15.4 : 1.0\n",
      "                violence = True           Counte : Hate   =     13.2 : 1.0\n",
      "                  racism = True           Counte : Hate   =     13.2 : 1.0\n",
      "                   asian = True           Counte : Hate   =     12.1 : 1.0\n",
      "                  target = True           Counte : Hate   =     11.9 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_data = get_dataset(train=True)\n",
    "test_data = get_dataset(test=True)\n",
    "    \n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
