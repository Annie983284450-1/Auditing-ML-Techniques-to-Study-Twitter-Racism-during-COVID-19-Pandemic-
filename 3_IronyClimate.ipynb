{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets  'tensorflow==2.15'"
      ],
      "metadata": {
        "id": "NOD0G-FLLpUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9ZpS1V0LjGB"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from datasets import load_dataset\n",
        "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, Layer, LayerNormalization, Add, MultiHeadAttention\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the TweetEval datasets\n",
        "ds_irony = load_dataset(\"cardiffnlp/tweet_eval\", \"irony\")\n",
        "ds_stance = load_dataset(\"cardiffnlp/tweet_eval\", \"stance_climate\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Preprocess and Tokenize the Data\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize_data(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "# Tokenize datasets\n",
        "ds_irony = ds_irony.map(tokenize_data, batched=True)\n",
        "ds_stance = ds_stance.map(tokenize_data, batched=True)\n",
        "\n",
        "# Step 3: Align Datasets for Multi-Output Training\n",
        "def align_datasets(ds1, ds2):\n",
        "    size = min(len(ds1[\"train\"][\"label\"]), len(ds2[\"train\"][\"label\"]))\n",
        "    ds1_inputs = {\n",
        "        \"input_ids\": np.array(ds1[\"train\"][\"input_ids\"][:size]),\n",
        "        \"attention_mask\": np.array(ds1[\"train\"][\"attention_mask\"][:size])\n",
        "    }\n",
        "    ds1_labels = tf.keras.utils.to_categorical(ds1[\"train\"][\"label\"][:size], num_classes=2)\n",
        "    ds2_labels = tf.keras.utils.to_categorical(ds2[\"train\"][\"label\"][:size], num_classes=3)\n",
        "    return ds1_inputs, ds1_labels, ds2_labels\n",
        "\n",
        "inputs, labels_irony, labels_stance = align_datasets(ds_irony, ds_stance)\n"
      ],
      "metadata": {
        "id": "2Bx4NrofLmEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 4: Custom DistilBERT Layer (Frozen)\n",
        "class DistilBERTLayer(Layer):\n",
        "    def __init__(self, model_name=\"distilbert-base-uncased\", **kwargs):\n",
        "        super(DistilBERTLayer, self).__init__(**kwargs)\n",
        "        self.distilbert = TFDistilBertModel.from_pretrained(model_name, trainable=False)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_ids = inputs[\"input_ids\"]\n",
        "        attention_mask = inputs[\"attention_mask\"]\n",
        "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        return outputs.last_hidden_state\n",
        "\n",
        "# Step 5: Custom Attention Block with Residual Connections\n",
        "def attention_feedforward_block(x, num_heads, feedforward_dim, dropout_rate):\n",
        "    # Multi-Head Attention\n",
        "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=x.shape[-1])(x, x)\n",
        "    attention_output = Dropout(dropout_rate)(attention_output)\n",
        "    attention_output = LayerNormalization(epsilon=1e-6)(x + attention_output)  # Residual connection + normalization\n",
        "\n",
        "    # Feedforward Network\n",
        "    ffn_output = Dense(feedforward_dim, activation=\"relu\")(attention_output)\n",
        "    ffn_output = Dense(x.shape[-1])(ffn_output)  # Match dimension back\n",
        "    ffn_output = Dropout(dropout_rate)(ffn_output)\n",
        "    output = LayerNormalization(epsilon=1e-6)(attention_output + ffn_output)  # Residual connection + normalization\n",
        "    return output\n",
        "\n",
        "# Step 6: Build the Multi-Output Model\n",
        "input_ids = Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\n",
        "attention_mask = Input(shape=(128,), dtype=tf.int32, name=\"attention_mask\")\n",
        "inputs_model = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
        "\n",
        "# DistilBERT base (frozen)\n",
        "distilbert_layer = DistilBERTLayer()\n",
        "bert_output = distilbert_layer(inputs_model)\n",
        "\n",
        "# Mean pooling\n",
        "pooled_output = tf.reduce_mean(bert_output, axis=1)\n",
        "\n",
        "# Add custom attention and feedforward layers with residuals\n",
        "x = attention_feedforward_block(bert_output, num_heads=4, feedforward_dim=1024, dropout_rate=0.3)\n",
        "x = attention_feedforward_block(x, num_heads=4, feedforward_dim=1024, dropout_rate=0.3)\n",
        "\n",
        "# Mean pooling after additional layers\n",
        "x = tf.reduce_mean(x, axis=1)\n",
        "\n",
        "# Dropout for regularization\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "# Task-specific output heads\n",
        "irony_output = Dense(2, activation=\"softmax\", name=\"irony_output\")(x)  # Binary classification\n",
        "stance_output = Dense(3, activation=\"softmax\", name=\"stance_output\")(x)  # Multi-class classification\n",
        "\n",
        "# Define the model\n",
        "multi_output_model = Model(inputs=[input_ids, attention_mask], outputs=[irony_output, stance_output])\n",
        "multi_output_model.summary()\n"
      ],
      "metadata": {
        "id": "oL5lcWODvGEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 7: Compile the Model\n",
        "multi_output_model.compile(\n",
        "    optimizer=Adam(learning_rate=2e-5),\n",
        "    loss={\n",
        "        \"irony_output\": \"categorical_crossentropy\",\n",
        "        \"stance_output\": \"categorical_crossentropy\"\n",
        "    },\n",
        "    metrics={\n",
        "        \"irony_output\": \"accuracy\",\n",
        "        \"stance_output\": \"accuracy\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Step 8: Train the Model\n",
        "history = multi_output_model.fit(\n",
        "    x={\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]},\n",
        "    y={\"irony_output\": labels_irony, \"stance_output\": labels_stance},\n",
        "    batch_size=64,\n",
        "    epochs=20,\n",
        "    validation_split=0.1\n",
        ")\n"
      ],
      "metadata": {
        "id": "Qf9MUW1_vJyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 9: Evaluate the Model\n",
        "results = multi_output_model.evaluate(\n",
        "    x={\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]},\n",
        "    y={\"irony_output\": labels_irony, \"stance_output\": labels_stance}\n",
        ")\n",
        "print(\"Evaluation Results:\", results)\n"
      ],
      "metadata": {
        "id": "D8SIoJq5vL_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 10: Make Predictions\n",
        "sample_texts = [\"This weather is fantastic... not!\", \"Climate change is real, and we must act now.\"]\n",
        "tokenized_inputs = tokenizer(sample_texts, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"tf\")\n",
        "\n",
        "predictions = multi_output_model.predict({\n",
        "    \"input_ids\": tokenized_inputs[\"input_ids\"],\n",
        "    \"attention_mask\": tokenized_inputs[\"attention_mask\"]\n",
        "})\n",
        "\n",
        "for i, text in enumerate(sample_texts):\n",
        "    irony_pred = predictions[0][i].argmax()\n",
        "    stance_pred = predictions[1][i].argmax()\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"  Predicted Irony: {'Ironic' if irony_pred == 1 else 'Not Ironic'}\")\n",
        "    print(f\"  Predicted Stance: {['Against', 'Neutral', 'Favor'][stance_pred]}\")\n"
      ],
      "metadata": {
        "id": "UqLV71TMvNjt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}